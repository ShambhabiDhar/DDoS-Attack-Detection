# -*- coding: utf-8 -*-
"""DDoS Attack Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S_E6wKrn5Yc0insgocG2nvd0B1ErBfj7
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import seaborn as sns

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score as f1

from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
from sklearn.mixture import GaussianMixture


dt = pd.read_csv('dataset_sdn.csv')

dt.columns

dt.dtypes.value_counts()

print(f"Number of NUMERIC features: 20 \n")
print(f"Number of OBJECT features: 3 \n")
# object datatype : string OR mixed;  (Can't be used in Regression)

dt.info()
#total number of data points= 10435; #attributes = 23 (inlcuding 'label')

dt.label.unique()
# binary label =, ie. '0' or '1'

"""Label :       MALICIOUS : 1; BENIGN : 0"""

dt.label.value_counts()

# # label_dict = dict(dt.label.value_counts())
# x = dt.label
# sns.countplot(arg: x)
# # label_dict

labels = ["Benign", "Malicious"] 
counts = [dt.label.value_counts()[0], dt.label.value_counts()[1]]
# plt.figure(figsize = (13,8))
plt.pie(counts, labels= labels, radius=1.1, colors=['red', 'purple'], labeldistance=1.1, autopct='%1.1f%%')
plt.legend()
plt.title("Percentage distribution of the data")
plt.show()

dt.isna().sum().plot.bar(color='black')
plt.title("MISSING VALUES")
plt.xlabel("Attribute")
plt.ylabel("Number of missing values")
plt.show()

# dt.src.value_counts().plot.barh(color='red')
plt.barh(dt.src.value_counts().keys(),list(dt.src.value_counts()), color='red')
plt.title("NUMBER OF REQUESTS FROM EACH IP")
plt.xlabel("Number of requests")
plt.ylabel("Source IPs")
plt.show()

plt.barh(dt[dt['label']==1].src.value_counts().keys(), list(dt[dt['label']==1].src.value_counts()), color='purple')
plt.title("NUMBER OF MALICIOUS REQUESTS FROM EACH IP")
plt.xlabel("Number of requests")
plt.ylabel("Source IPs")
plt.show()

plt.barh(dt.src.value_counts().keys(),list(dt.src.value_counts()), color='red')
plt.barh(dt[dt['label']==1].src.value_counts().keys(), list(dt[dt['label']==1].src.value_counts()), color='purple')
plt.legend(['TOTAL', "MALICIOUS"])
plt.title("FRACTION OF MALICIOUS REQUESTS FROM EACH IP")
plt.xlabel("Number of requests")
plt.ylabel("Source IPs")
plt.show()

plt.bar(dt.Protocol.value_counts().keys(), list(dt.Protocol.value_counts()), color='red')
plt.bar(dt[dt['label']==1].Protocol.value_counts().keys(), list(dt[dt['label']==1].Protocol.value_counts()), color='purple')
plt.show()

plt.hist(dt.dur, bins=20, color='PURPLE')
plt.title('HISTOGRAM OF DURATION ATTRIBUTE')
plt.show()

dt_0 = dt.copy()
dt_0.dropna(inplace=True)
#dropping the NULL values
dumb_dt = pd.get_dummies(dt_0)
st = StandardScaler()
st.fit(dumb_dt)
dt_1 = st.transform(dumb_dt)
dt_1 = pd.DataFrame(dt_1)
dt_1.columns = dumb_dt.columns
dt_1.drop(['label'], axis=1, inplace=True)
X_train, X_test, Y_train, Y_test = train_test_split(dt_1, dt_0.label, random_state=42, test_size=0.3)

solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']
results_lr = []
acc_l = []
f1_l = []
for solver in solvers:
    lr = LogisticRegression(C=0.03, solver=solver).fit(X_train,Y_train)
    pred_lr = lr.predict(X_test)
    acc_lr = accuracy_score(Y_test, pred_lr)
    print(f"Accuracy score using {solver} solver: {acc_lr}\n")
    results_lr.append({'solver' : solver, 'accuracy score': round(acc_lr, 6), 'Coefficients': {'W' : lr.coef_, 'b': lr.intercept_}})
    acc_l.append(acc_lr)
    f1_l.append(f1(Y_test, pred_lr))
best_solver = solvers[acc_l.index(max(acc_l))]
lr = LogisticRegression(C=0.03, solver=best_solver).fit(X_train, Y_train)
pred_lr = lr.predict(X_test)
acc_lr = accuracy_score(Y_test, pred_lr)
print(f"Accuracy score of Logistic Regression using the best solver '{best_solver}': {acc_lr}\n")
print(f"CLASSIFICATION REPORT:\n {classification_report(pred_lr, Y_test)}")
maxacc_lr = max(acc_l)
maxf1_lr  = max(f1_l)

Q = [2]
acc_gmm = []
f1s_gmm = []
for i in Q:
    gm=GaussianMixture(n_components=i)
    gm.fit(X_train, Y_train)
    pred_gmm=gm.predict(X_test)
    acc_gmm.append(accuracy_score(Y_test,pred_gmm))
    f1s_gmm.append(f1(Y_test, pred_gmm))
print(f"Maximum accuracy score using GMM: {max(acc_gmm)}\n")
print(f"CLASSIFICATION REPORT:\n{classification_report(pred_gmm, Y_test)}")
maxacc_gmm = max(acc_gmm)
maxf1_gmm = max(f1s_gmm)

K=[5,7,13,19]
acc_l=[]
f1l=[]
for i in K:
    knn= KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train, Y_train)
    pred_knn=knn.predict(X_test)
    acc_l.append(accuracy_score(Y_test,pred_knn))
    f1l.append(f1(Y_test,pred_knn))
print(f"Maximum accurcay score for KNN: {max(acc_l)}\n")
print(f"CLASSIFICATION REPORT:\n{classification_report(pred_knn, Y_test)}")
maxacc_knn = max(acc_l)
maxf1_knn = max(f1l)

RF = RandomForestClassifier(n_jobs=-1, n_estimators=500, min_samples_split=10, criterion='gini',max_features='auto',oob_score=True,random_state=1)
RF.fit(X_train, Y_train)
pred_rf = RF.predict(X_test)
acc_rf = accuracy_score(Y_test, pred_rf)
print(f"Accuracy score for Random Forest: {acc_rf}\n")
print(f"CLASSIFICATION REPORT:\n{classification_report(pred_rf, Y_test)}")
maxacc_rf = acc_rf
maxf1_rf = f1(Y_test, pred_rf)

dt_2 = dt_1.copy()
comp = [2, 5, 10, 13, 15, 19, 21]
max_acc = []
for i in comp:
    pca = PCA(n_components=i)
    pca.fit(dt_2)
    pca_dt = pca.transform(dt_2)
    X_train, X_test, Y_train, Y_test = train_test_split(pca_dt, dumb_dt.label, random_state=42, test_size=0.3)
    solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']
    results_lr = []
    acc_l = []
    for solver in solvers:
        lr = LogisticRegression(C=0.03, solver=solver).fit(X_train,Y_train)
        pred_lr = lr.predict(X_test)
        acc_lr = accuracy_score(Y_test, pred_lr)
        acc_l.append(acc_lr)
    max_acc.append(max(acc_l))
print(f"Maximum Accuracy score using PCA for 3 components = {max(max_acc)}")

"""PCA REDUCED DATA IS GIVING VERY LOW ACCURACY. HENCE, WE HAVE DONE CLASSIFICATION WITHOUT DATA REDUCTION IS BETTER"""

maxacc = [maxacc_lr, maxacc_gmm, maxacc_knn, maxacc_rf]
plt.bar(['Logistic Regression', 'GMM', 'KNN', 'Random Forest'], maxacc)
plt.title("COMPARISION OF ACCURACY SCORES")
plt.ylabel("Accuracy Score")
plt.show()

maxf1 = [maxf1_lr, maxf1_gmm, maxf1_knn, maxf1_rf]
plt.bar(['Logistic Regression', 'GMM', 'KNN', 'Random Forest'], maxf1)
plt.title("COMPARISION OF F1-SCORES")
plt.ylabel("F1-Score")
plt.show()

"""RANDOM FOREST IS BEST ALGORITHM AS IT HAS THE MAXIMUM ACCURACY SCORE COMPARISION"""

